{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled1.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vedangi2610/FHE-project-/blob/master/data_collection/CollectMaxTweets.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G6vSpa805OAU"
      },
      "source": [
        "<h1>Current Data Collection Code</h1>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hB7dCfc6mj1S"
      },
      "source": [
        "import tweepy #https://github.com/tweepy/tweepy\n",
        "import csv\n",
        "import pandas as pd"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "abfqu_uTmlSn"
      },
      "source": [
        "#Twitter API credentials\n",
        "consumer_key = \"kB05ueHmnwXSqQbah9QBETV5s\"\n",
        "consumer_secret = \"8SBnQwZHGthoKIA50231K46j4kiRdLqVs54WXvn3t0h6xYKd0Q\"\n",
        "access_key = \"899207374048710656-J0xFuFaHQ8mv0EJBsXOTxaSAyRjs2UP\"\n",
        "access_secret = \"cvBFKYoW9zUqkIavedyEj8Wte0r2UnXRNCwVJyT0apjmw\""
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yx0h6i89m9V9"
      },
      "source": [
        "def get_all_tweets(screen_name, file_name):\n",
        "    #Twitter only allows access to a users most recent 3240 tweets with this method\n",
        "    \n",
        "    #authorize twitter, initialize tweepy\n",
        "    auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
        "    auth.set_access_token(access_key, access_secret)\n",
        "    api = tweepy.API(auth)\n",
        "    \n",
        "    #initialize a list to hold all the tweepy Tweets\n",
        "    alltweets = []  \n",
        "    \n",
        "    #make initial request for most recent tweets (200 is the maximum allowed count)\n",
        "    new_tweets = api.user_timeline(screen_name = screen_name,count=200)\n",
        "    \n",
        "    #save most recent tweets\n",
        "    alltweets.extend(new_tweets)\n",
        "    \n",
        "    #save the id of the oldest tweet less one\n",
        "    oldest = alltweets[-1].id - 1\n",
        "    \n",
        "    #keep grabbing tweets until there are no tweets left to grab\n",
        "    while len(new_tweets) > 0:\n",
        "        #print(f\"getting tweets before {oldest}\")\n",
        "        \n",
        "        #all subsiquent requests use the max_id param to prevent duplicates\n",
        "        new_tweets = api.user_timeline(screen_name = screen_name,count=200,max_id=oldest)\n",
        "        \n",
        "        #save most recent tweets\n",
        "        alltweets.extend(new_tweets)\n",
        "        \n",
        "        #update the id of the oldest tweet less one\n",
        "        oldest = alltweets[-1].id - 1\n",
        "        \n",
        "        #print(f\"...{len(alltweets)} tweets downloaded so far\")\n",
        "    \n",
        "    #transform the tweepy tweets into a 2D array that will populate the csv \n",
        "    outtweets = [[tweet.id_str, screen_name, tweet.created_at, tweet.lang, tweet.favorite_count, tweet.retweet_count, tweet.text] for tweet in alltweets]\n",
        "   \n",
        "    '''Code to add to a new file for each screen_name '''\n",
        "#     with open(f'new_{screen_name}_tweets.csv', 'w',  encoding=\"utf-8\") as f:\n",
        "#         writer = csv.writer(f)\n",
        "#         writer.writerow([\"id\",\"username\",\"created_at\",\"lang\",\"favorite_count\",\"retweet_count\",\"text\"])\n",
        "#         writer.writerows(outtweets)\n",
        "    \n",
        "    '''Code for csv file addition'''\n",
        "    # Open/create a file to append data, This will add data to the file Not create a new file each run\n",
        "    file_name = file_name+'.csv'        \n",
        "    csvFile = open(file_name, 'a', encoding=\"utf-8\")\n",
        "    csvWriter = csv.writer(csvFile)\n",
        "    '''To check if file exists'''\n",
        "    try: \n",
        "        pd.read_csv(file_name)\n",
        "    except pd.errors.EmptyDataError:\n",
        "        csvWriter.writerow([\"id\",\"username\",\"created_at\",\"lang\",\"favorite_count\",\"retweet_count\",\"text\"])\n",
        "        \n",
        "    csvWriter.writerows(outtweets)\n",
        "    csvFile.close()\n",
        "\n",
        "    pass\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Y8Yvu0w5OAj"
      },
      "source": [
        "def get_tweets_all_topics():\n",
        "    topic1=[\"narendramodi\", \"ShashiTharoor\", \"JoeBiden\", \"AtishiAAP\"]\n",
        "    Entertainmentarr=[\"tomhanks\"]\n",
        "    ScienceandTecharr=[ \"TheNextWeb\", \"techreview\"]\n",
        "    #\"TheScienceGuy\",\n",
        "    Entrepreneurshiparr=[\"hnshah\", \"ChrisDucker\", \"KauffmanFDN\", \"richardbranson\", \"randizuckerberg\"]\n",
        "    Healthcarearr=[\"DianeEMeier\", \"EricTopol\", \"aetiology\", \"medpagetoday\"]\n",
        "    Sportsarr=[\"SInow\", \"SkySportsPL\", \"SportingLife\", \"SportsIndia3\"]\n",
        "    #topic_names=[\"Politics\",\"Entertainment\", \"ScienceandTech\", \"Entrepreneurship\", \"Healthcare\", \"Sports\"]\n",
        "    topic_dictionary={\"Politics\":topic1,\"Entertainment\":Entertainmentarr,\"ScienceandTech\":ScienceandTecharr, \"Entrepreneurship\":Entrepreneurshiparr, \"Healthcare\":Healthcarearr, \"Sports\":Sportsarr}\n",
        "    \n",
        "    for t1 in topic_dictionary.keys():\n",
        "        print(t1)\n",
        "        for t2 in topic_dictionary.get(t1):\n",
        "          print(t2)\n",
        "          try:\n",
        "            get_all_tweets(t2, t1)\n",
        "          except tweepy.TweepError as e: \n",
        "            print(\"Tweepy Error: {}\".format(e))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HHLqetoWiIgn"
      },
      "source": [
        "def get_all_UserTweets(screen_name, file_name):\n",
        "    #Twitter only allows access to a users most recent 3240 tweets with this method\n",
        "    \n",
        "    #authorize twitter, initialize tweepy\n",
        "    auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
        "    auth.set_access_token(access_key, access_secret)\n",
        "    api = tweepy.API(auth)\n",
        "  \n",
        "    # fetching the user\n",
        "    user = api.get_user(screen_name)\n",
        "      \n",
        "    # fetching the statuses_count attribute\n",
        "    statuses_count = user.statuses_count \n",
        "      \n",
        "    #print(\"The number of statuses the user has posted are : \" + str(statuses_count))\n",
        "    \n",
        "    if(statuses_count>1000):\n",
        "    \n",
        "          #initialize a list to hold all the tweepy Tweets\n",
        "          alltweets = []  \n",
        "          \n",
        "          #make initial request for most recent tweets (200 is the maximum allowed count)\n",
        "          new_tweets = api.user_timeline(screen_name = screen_name,count=200)\n",
        "          \n",
        "          #save most recent tweets\n",
        "          alltweets.extend(new_tweets)\n",
        "          \n",
        "          #save the id of the oldest tweet less one\n",
        "          oldest = alltweets[-1].id - 1\n",
        "          \n",
        "          cnt = 0\n",
        "          #keep grabbing tweets until there are no tweets left to grab\n",
        "          while len(new_tweets) > 0 and cnt<5:\n",
        "              #print(f\"getting tweets before {oldest}\")\n",
        "              \n",
        "              #all subsiquent requests use the max_id param to prevent duplicates\n",
        "              new_tweets = api.user_timeline(screen_name = screen_name,count=200,max_id=oldest)\n",
        "              \n",
        "              #save most recent tweets\n",
        "              alltweets.extend(new_tweets)\n",
        "              \n",
        "              #update the id of the oldest tweet less one\n",
        "              oldest = alltweets[-1].id - 1\n",
        "              cnt = cnt+1\n",
        "              #print(f\"...{len(alltweets)} tweets downloaded so far\")\n",
        "              \n",
        "          #transform the tweepy tweets into a 2D array that will populate the csv \n",
        "          outtweets = [[tweet.id_str, screen_name, tweet.created_at, tweet.lang, tweet.favorite_count, tweet.retweet_count, tweet.text] for tweet in alltweets]\n",
        "        \n",
        "          '''Code to add to a new file for each screen_name '''\n",
        "      #     with open(f'new_{screen_name}_tweets.csv', 'w',  encoding=\"utf-8\") as f:\n",
        "      #         writer = csv.writer(f)\n",
        "      #         writer.writerow([\"id\",\"username\",\"created_at\",\"lang\",\"favorite_count\",\"retweet_count\",\"text\"])\n",
        "      #         writer.writerows(outtweets)\n",
        "          \n",
        "          '''Code for csv file addition'''\n",
        "          # Open/create a file to append data, This will add data to the file Not create a new file each run\n",
        "          file_name = file_name+'.csv'        \n",
        "          csvFile = open(file_name, 'a', encoding=\"utf-8\")\n",
        "          csvWriter = csv.writer(csvFile)\n",
        "          '''To check if file exists'''\n",
        "          try: \n",
        "              pd.read_csv(file_name)\n",
        "          except pd.errors.EmptyDataError:\n",
        "              csvWriter.writerow([\"id\",\"username\",\"created_at\",\"lang\",\"favorite_count\",\"retweet_count\",\"text\"])\n",
        "              \n",
        "          csvWriter.writerows(outtweets)\n",
        "          csvFile.close()\n",
        "\n",
        "          pass\n",
        "    \n",
        "    else:\n",
        "      print(\"The numbers of  tweets are insufficient for \"+screen_name)\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zP1HEALsnk9z"
      },
      "source": [
        "if __name__ == '__main__':\n",
        "    #pass in the username of the account you want to download\n",
        "    #get_all_tweets(\"weedbiryani\", \"test_csv\")\n",
        "    #get_tweets_all_topics()\n",
        "    #print(\"Done!\")\n",
        "    get_all_UserTweets(\"ShekharGupta\", \"test_csv\")"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IoR9r3vB9wpI"
      },
      "source": [
        "\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uKzvVSij5yNo"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}