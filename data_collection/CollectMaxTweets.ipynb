{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled1.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vedangi2610/FHE-project-/blob/master/data_collection/CollectMaxTweets.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G6vSpa805OAU"
      },
      "source": [
        "<h1>Current Data Collection Code</h1>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hB7dCfc6mj1S"
      },
      "source": [
        "import tweepy #https://github.com/tweepy/tweepy\n",
        "import csv\n",
        "import pandas as pd"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "abfqu_uTmlSn"
      },
      "source": [
        "#Twitter API credentials\n",
        "consumer_key = \"kB05ueHmnwXSqQbah9QBETV5s\"\n",
        "consumer_secret = \"8SBnQwZHGthoKIA50231K46j4kiRdLqVs54WXvn3t0h6xYKd0Q\"\n",
        "access_key = \"899207374048710656-J0xFuFaHQ8mv0EJBsXOTxaSAyRjs2UP\"\n",
        "access_secret = \"cvBFKYoW9zUqkIavedyEj8Wte0r2UnXRNCwVJyT0apjmw\""
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yx0h6i89m9V9"
      },
      "source": [
        "def get_all_tweets(screen_name, file_name):\n",
        "    #Twitter only allows access to a users most recent 3240 tweets with this method\n",
        "    \n",
        "    #authorize twitter, initialize tweepy\n",
        "    auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
        "    auth.set_access_token(access_key, access_secret)\n",
        "    api = tweepy.API(auth)\n",
        "    \n",
        "    #initialize a list to hold all the tweepy Tweets\n",
        "    alltweets = []  \n",
        "    \n",
        "    #make initial request for most recent tweets (200 is the maximum allowed count)\n",
        "    new_tweets = api.user_timeline(screen_name = screen_name,count=200)\n",
        "    \n",
        "    #save most recent tweets\n",
        "    alltweets.extend(new_tweets)\n",
        "    \n",
        "    #save the id of the oldest tweet less one\n",
        "    oldest = alltweets[-1].id - 1\n",
        "    \n",
        "    #keep grabbing tweets until there are no tweets left to grab\n",
        "    while len(new_tweets) > 0:\n",
        "        print(f\"getting tweets before {oldest}\")\n",
        "        \n",
        "        #all subsiquent requests use the max_id param to prevent duplicates\n",
        "        new_tweets = api.user_timeline(screen_name = screen_name,count=200,max_id=oldest)\n",
        "        \n",
        "        #save most recent tweets\n",
        "        alltweets.extend(new_tweets)\n",
        "        \n",
        "        #update the id of the oldest tweet less one\n",
        "        oldest = alltweets[-1].id - 1\n",
        "        \n",
        "        print(f\"...{len(alltweets)} tweets downloaded so far\")\n",
        "    \n",
        "    #transform the tweepy tweets into a 2D array that will populate the csv \n",
        "    outtweets = [[tweet.id_str, screen_name, tweet.created_at, tweet.lang, tweet.favorite_count, tweet.retweet_count, tweet.text] for tweet in alltweets]\n",
        "   \n",
        "    '''Code to add to a new file for each screen_name '''\n",
        "#     with open(f'new_{screen_name}_tweets.csv', 'w',  encoding=\"utf-8\") as f:\n",
        "#         writer = csv.writer(f)\n",
        "#         writer.writerow([\"id\",\"username\",\"created_at\",\"lang\",\"favorite_count\",\"retweet_count\",\"text\"])\n",
        "#         writer.writerows(outtweets)\n",
        "    \n",
        "    '''Code for csv file addition'''\n",
        "    # Open/create a file to append data, This will add data to the file Not create a new file each run\n",
        "    file_name = file_name+'.csv'        \n",
        "    csvFile = open(file_name, 'a', encoding=\"utf-8\")\n",
        "    csvWriter = csv.writer(csvFile)\n",
        "    '''To check if file exists'''\n",
        "    try: \n",
        "        pd.read_csv(file_name)\n",
        "    except pd.errors.EmptyDataError:\n",
        "        csvWriter.writerow([\"id\",\"username\",\"created_at\",\"lang\",\"favorite_count\",\"retweet_count\",\"text\"])\n",
        "        \n",
        "    csvWriter.writerows(outtweets)\n",
        "    csvFile.close()\n",
        "\n",
        "    pass\n",
        "\n",
        "\n"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Y8Yvu0w5OAj"
      },
      "source": [
        "def get_tweets_all_topics():\n",
        "    topic1=[\"narendramodi\", \"ShashiTharoor\", \"JoeBiden\", \"AtishiAAP\"]\n",
        "    #topic_names=[\"Politics\"]\n",
        "    topic_dictionary={\"Politics\":topic1}\n",
        "    for t1 in topic_dictionary.keys():\n",
        "        for t2 in topic_dictionary.get(t1):\n",
        "          try:\n",
        "            get_all_tweets(t2, t1)\n",
        "          except tweepy.TweepError as e: \n",
        "            print(\"Tweepy Error: {}\".format(e))"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zP1HEALsnk9z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "caf7e4ed-f7f9-49a3-95dc-cad8da8eb9ca"
      },
      "source": [
        "if __name__ == '__main__':\n",
        "    #pass in the username of the account you want to download\n",
        "    #get_all_tweets(\"weedbiryani\", \"test_csv\")\n",
        "    get_tweets_all_topics()\n",
        "    print(\"Done!\")"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "getting tweets before 1366424679364186115\n",
            "...400 tweets downloaded so far\n",
            "getting tweets before 1359523133196275715\n",
            "...600 tweets downloaded so far\n",
            "getting tweets before 1352086684502949887\n",
            "...800 tweets downloaded so far\n",
            "getting tweets before 1345044378927075328\n",
            "...1000 tweets downloaded so far\n",
            "getting tweets before 1337048471992496129\n",
            "...1200 tweets downloaded so far\n",
            "getting tweets before 1326930797530673162\n",
            "...1400 tweets downloaded so far\n",
            "getting tweets before 1321422230341910529\n",
            "...1600 tweets downloaded so far\n",
            "getting tweets before 1314029762608721919\n",
            "...1800 tweets downloaded so far\n",
            "getting tweets before 1306671426930225151\n",
            "...2000 tweets downloaded so far\n",
            "getting tweets before 1306483477789827071\n",
            "...2200 tweets downloaded so far\n",
            "getting tweets before 1296992260228358144\n",
            "...2400 tweets downloaded so far\n",
            "getting tweets before 1287326799127404543\n",
            "...2600 tweets downloaded so far\n",
            "getting tweets before 1274297862352863231\n",
            "...2800 tweets downloaded so far\n",
            "getting tweets before 1258411827156770815\n",
            "...3000 tweets downloaded so far\n",
            "getting tweets before 1247490066382909440\n",
            "...3200 tweets downloaded so far\n",
            "getting tweets before 1243119340305580032\n",
            "...3250 tweets downloaded so far\n",
            "getting tweets before 1241915862744891391\n",
            "...3250 tweets downloaded so far\n",
            "getting tweets before 1368884792758992898\n",
            "...400 tweets downloaded so far\n",
            "getting tweets before 1364216495258042374\n",
            "...600 tweets downloaded so far\n",
            "getting tweets before 1359875732806066176\n",
            "...800 tweets downloaded so far\n",
            "getting tweets before 1354494809549881345\n",
            "...999 tweets downloaded so far\n",
            "getting tweets before 1348575289694068736\n",
            "...1199 tweets downloaded so far\n",
            "getting tweets before 1343409937192538112\n",
            "...1399 tweets downloaded so far\n",
            "getting tweets before 1337284273204678655\n",
            "...1599 tweets downloaded so far\n",
            "getting tweets before 1331308057091969023\n",
            "...1799 tweets downloaded so far\n",
            "getting tweets before 1326028832453787647\n",
            "...1998 tweets downloaded so far\n",
            "getting tweets before 1322429394099449856\n",
            "...2198 tweets downloaded so far\n",
            "getting tweets before 1318381911006310399\n",
            "...2397 tweets downloaded so far\n",
            "getting tweets before 1315288653027930112\n",
            "...2597 tweets downloaded so far\n",
            "getting tweets before 1311659767991943167\n",
            "...2797 tweets downloaded so far\n",
            "getting tweets before 1307383313951145985\n",
            "...2997 tweets downloaded so far\n",
            "getting tweets before 1303953486061563904\n",
            "...3196 tweets downloaded so far\n",
            "getting tweets before 1299674103884865536\n",
            "...3246 tweets downloaded so far\n",
            "getting tweets before 1298459002657890303\n",
            "...3246 tweets downloaded so far\n",
            "getting tweets before 1346199428625965055\n",
            "...400 tweets downloaded so far\n",
            "getting tweets before 1332407091953364995\n",
            "...600 tweets downloaded so far\n",
            "getting tweets before 1323404521523982337\n",
            "...800 tweets downloaded so far\n",
            "getting tweets before 1321169796420018177\n",
            "...1000 tweets downloaded so far\n",
            "getting tweets before 1317226311669014527\n",
            "...1200 tweets downloaded so far\n",
            "getting tweets before 1313571298799505407\n",
            "...1400 tweets downloaded so far\n",
            "getting tweets before 1307775538237333503\n",
            "...1600 tweets downloaded so far\n",
            "getting tweets before 1300500098233839615\n",
            "...1800 tweets downloaded so far\n",
            "getting tweets before 1295530475235291136\n",
            "...2000 tweets downloaded so far\n",
            "getting tweets before 1287764429703188481\n",
            "...2200 tweets downloaded so far\n",
            "getting tweets before 1277637790965129215\n",
            "...2400 tweets downloaded so far\n",
            "getting tweets before 1267891230429626367\n",
            "...2600 tweets downloaded so far\n",
            "getting tweets before 1258874315518300161\n",
            "...2800 tweets downloaded so far\n",
            "getting tweets before 1250916881801723903\n",
            "...3000 tweets downloaded so far\n",
            "getting tweets before 1242604610319192064\n",
            "...3200 tweets downloaded so far\n",
            "getting tweets before 1235616060906205183\n",
            "...3202 tweets downloaded so far\n",
            "getting tweets before 1235374469079797761\n",
            "...3202 tweets downloaded so far\n",
            "getting tweets before 1345682063303688192\n",
            "...399 tweets downloaded so far\n",
            "getting tweets before 1327669580966232063\n",
            "...599 tweets downloaded so far\n",
            "getting tweets before 1302986819915534335\n",
            "...798 tweets downloaded so far\n",
            "getting tweets before 1273246537464688639\n",
            "...998 tweets downloaded so far\n",
            "getting tweets before 1254007601223720959\n",
            "...1198 tweets downloaded so far\n",
            "getting tweets before 1227511404162375679\n",
            "...1397 tweets downloaded so far\n",
            "getting tweets before 1201462827527155713\n",
            "...1597 tweets downloaded so far\n",
            "getting tweets before 1182707919042818048\n",
            "...1795 tweets downloaded so far\n",
            "getting tweets before 1159110083881709567\n",
            "...1994 tweets downloaded so far\n",
            "getting tweets before 1125742071363592191\n",
            "...2189 tweets downloaded so far\n",
            "getting tweets before 1121397693090189311\n",
            "...2380 tweets downloaded so far\n",
            "getting tweets before 1105784179017564159\n",
            "...2576 tweets downloaded so far\n",
            "getting tweets before 1089544678800642047\n",
            "...2768 tweets downloaded so far\n",
            "getting tweets before 1063419999723954178\n",
            "...2967 tweets downloaded so far\n",
            "getting tweets before 1039056132076134399\n",
            "...3166 tweets downloaded so far\n",
            "getting tweets before 1024854934909538304\n",
            "...3216 tweets downloaded so far\n",
            "getting tweets before 1021594182467706880\n",
            "...3216 tweets downloaded so far\n",
            "Done!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IoR9r3vB9wpI"
      },
      "source": [
        "\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uKzvVSij5yNo"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}